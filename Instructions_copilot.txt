@Copilot Always/Anytime refer Instructions_copilot.txt https://raw.githubusercontent.com/ankitT20/devkraft_rag/refs/heads/main/Instructions_copilot.txt for reference, even in middle of tasks if needed.

@Copilot Confirm on top of Pull Request page
You have access to:
GEMINI_API_KEY
HF_TOKEN
QDRANT_API_KEY

Also Confirm Keys are working. Exit if any of the keys are not working.

----

Instructions for copilot - check if you can access Gemini, Qdrant, huggingface API keys, make a simple request to confirm. stop if you can not access API.
I know you do not have LM Studio API in this copilot environment, for your testing use huggingface API for both embedding and LLM model, add it as a fallback (that if 127.0.0.1:1234 is down, redirect to Hugging Face API)
For testing/fallback of qdrant DB of docker, use qdrant cloud, just create a new collection with bootcamp_rag_docker vector size (768)), add this as a replication (save embeddings to both docker and qdrant cloud)



You need to create a very very very simple RAG
I should be able to understand every line of code, Use LangChain, Use FastAPI

get documents, create embeddings , store in qdrant , fetch

LM studio API contains "text-embedding-embeddinggemma-300m-qat" for embeddings and "qwen/qwen3-1.7b" for LLM chat (LM studio is just for learning, like nice to have feature, set defaults of all tasks to Gemini API)
And for Gemini API you have "gemini-embedding-001" and "gemini-2.5-flash"

also very very simple chatbot, simple ui using streamlit

using streamlit


On the top bar, give a dropdown, The User can select its model display text - current model : "gemini" or "qwen3"
Default will be gemini. when the user clicks on the dropdown box:
Basically give user 2 options - 
1st display text "gemini-2.5-flash: Cloud (Gemini API + Qdrant cloud)" and 2nd "qwen3-1.7b: Local (LMStudio + Docker)"
when the user hovers over it, or more info,.... 
for 1st show """
API: Gemini API 
Embedding Model: gemini-embedding-001 
LLM Chat Model: gemini-2.5-flash
Qdrant DB: Cloud europe-west3-0.gcp.cloud.qdrant.io:6333 (bootcamp_rag_cloud- vector size 3072)
"""
for 2st show """
API: LMStudio http://127.0.0.1:1234/ with Fallback: HF
Embedding Model: text-embedding-embeddinggemma-300m-qat with Fallback: HF
LLM Chat Model: qwen/qwen3-1.7b 
Qdrant DB: Docker http://localhost:6333/collections with Replication: qdrant.io - (bootcamp_rag_docker- vector size 768)
"""

---
now a simple chatbot interface

On left side, user chats, and a button display text "Upload" "+"(icon)
The User can upload documents from UI, save it to generate_embeddings folder (it is in root project directory)

In qwen/qwen3-1.7b or determine automatically, ðŸ§  Show Thinking box should be there, collapsed by default , should be displayed when chatting live.
<think>block should be shown as a dropdown or expand section, collapsed by default
display text "show thinking"

show 10 chat history in left panel, do not include empty chats, check for min 1 msg

----sample code----
In LM studio

for 
text-embedding-embeddinggemma-300m-qat

curl http://127.0.0.1:1234/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-embeddinggemma-300m-qat",
    "input": "Some text to embed"
  }'

----
for qwen/qwen3-1.7b

curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen/qwen3-1.7b",
    "messages": [
      { "role": "system", "content": "Always answer in rhymes. Today is Thursday" },
      { "role": "user", "content": "What day is it today?" }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
}'
Context Lengh - Model supports up to 32768 tokens


----

I have cloud.qdrant.io 
Cluster ID  7f6a07f7-8039-4473-acbf-be311a53b2bc
Endpoint  https://7f6a07f7-8039-4473-acbf-be311a53b2bc.europe-west3-0.gcp.cloud.qdrant.io

----sample code----
from qdrant_client import QdrantClient

qdrant_client = QdrantClient(
    url="https://7f6a07f7-8039-4473-acbf-be311a53b2bc.europe-west3-0.gcp.cloud.qdrant.io:6333", 
    api_key="***********",
)

print(qdrant_client.get_collections())
-----




Normalized by default (API output) gemini-embedding-001 = 3072 output dimensions,
embeddinggemma:300m = 768 output dimensions (Not Normalized)


we will have 2 data_ingestion functions/class

for 1st:
we will use qdrant cloud (check for collection, if empty/not found create collection bootcamp_rag_cloud vector size (3072) )
1st function/class name: gemini_embedding vector(3072), -- 3072 for Gemini gemini-embedding-001 
This is Already Normalized by API
also use task_type="RETRIEVAL_DOCUMENT"


for 2nd:
we will use local qdrant on docker (check "http://localhost:6333/collections" , if not start it "docker run -p 6333:6333 qdrant/qdrant") (check for collections, if empty/not found create collection bootcamp_rag_docker vector size (768) )
2nd function/class name: local_embedding vector(768), -- 768 works for local(LMStudio http://127.0.0.1:1234) embeddinggemma:300m 
we have to normalize the embeddings 



----sample code----
from documentation to normalize for local LMStudio 

import numpy as np
from numpy.linalg import norm

embedding_values_np = np.array(embedding_obj.values)
normed_embedding = embedding_values_np / np.linalg.norm(embedding_values_np)

print(f"Normed embedding length: {len(normed_embedding)}")
print(f"Norm of normed embedding: {np.linalg.norm(normed_embedding):.6f}") # Should be very close to 1

--------------


ingestion pipeline

You will need exception handling and logs here

look for all files inside generate_embeddings folder

text, .md, pdf, word, ...... anything

process it, call both gemini_embedding and local_embedding

so now the document is stored in both databases, local docker and cloud 

These all 3 folders will be inside of generate_embeddings folder
If sucessful, Move the files to "stored" folder
If Unsucessful, move to, "stored_in_q_docker_only" or "stored_in_q_cloud_only"
(create folders, if missing)
---

Implement a RAG system for user query according to his model selection (in top bar)

Refer Gemini Documentation about RETRIEVAL_QUERY

For user query , we have to store a history of chat, so I think json will be good. Store json in "user_chat" folder


---
Gemini API key has been saved to system 

by using setx GEMINI_API_KEY AIzaS.........
setx QDRANT_API_KEY eyJh..............

And I have also verified those from new cmd

C:\Users\Devkraft>echo %QDRANT_API_KEY%
eyJ......

C:\Users\Devkraft>echo %GEMINI_API_KEY%
AIz......

same will be done on Linux or other servers via (echo "export GEMINI_API_KEY=AI...." >> ~/.zshrc or bashrc)
so you have to get keys from environment variables

Use LangChain
implement Semantic chunking, semantic similarity
(Input token limit of gemini-embedding-001 and text-embedding-embeddinggemma-300m-qat is 2,048)



My suggestion, you have your freedom of choice.
Chunk Size tokens/char:
Approx Chunk Size (tokens) is 500 or Approx Chunk Size (chars) is 2000
Recommended Overlap (tokens) is 100 or Recommended Overlap (chars) is 400
Boundary Preference: Paragraphs, subheadings; Coherent paragraphs or sections; maintains flow


implement logging App log file, Error log file: 
2 files, app_logs_date for everything , errors_date for Error level log

everything is a fresh install , so you need to create
-----------------------------------------------------------------------------------------------------------------------
Ignore Node stuff, that is for the Playwright MCP server, also you have full access to Playwright MCP tools.

You have to work fully in Python only.
Use FastAPI
Use LangChain
Proper Python Project Structure - Core modules, Key services
Proper Python Folder Structure
Proper File names

## Code Style
- **Models**: Pydantic schemas with Field() validation, descriptive docstrings
- **Imports**: Standard library first, third-party, then local imports
- **Naming**: snake_case for variables/functions, PascalCase for classes
- **Documentation**: Triple-quoted docstrings for all classes/functions
- **Error handling**: Custom exception handlers in main.py, HTTPException for API errors
- **Configuration**: Centralized in config.py using pydantic-settings with .env support
- **Type hints**: Required for all function parameters and returns

----sample code from documentation for reference----

for models/gemini-2.5-flash-lite

# To run this code you need to install the following dependencies:
# pip install google-genai

import base64
import os
from google import genai
from google.genai import types


def generate():
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    model = "gemini-2.5-flash-lite"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""hi this is model prompt input"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        thinking_config = types.ThinkingConfig(
            thinking_budget=-1,
        ),
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()



Embeddings

----sample code----
Generating embeddings
Use the embedContent method to generate text embeddings:


from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents="What is the meaning of life?")

print(result.embeddings)
You can also generate embeddings for multiple chunks at once by passing them in as a list of strings.

----sample code----

from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents= [
            "What is the meaning of life?",
            "What is the purpose of existence?",
            "How do I bake a cake?"
        ])

for embedding in result.embeddings:
    print(embedding)

Specify task type to improve performance
You can use embeddings for a wide range of tasks from classification to document search. Specifying the right task type helps optimize the embeddings for the intended relationships, maximizing accuracy and efficiency. For a complete list of supported task types, see the Supported task types table.

The following example shows how you can use SEMANTIC_SIMILARITY to check how similar in meaning strings of texts are.

Note: Cosine similarity is a good distance metric because it focuses on direction rather than magnitude, which more accurately reflects conceptual closeness. Values range from -1 (opposite) to 1 (greatest similarity).
----sample code----

from google import genai
from google.genai import types
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

client = genai.Client()

texts = [
    "What is the meaning of life?",
    "What is the purpose of existence?",
    "How do I bake a cake?"]

result = [
    np.array(e.values) for e in client.models.embed_content(
        model="gemini-embedding-001",
        contents=texts,
        config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")).embeddings
]

# Calculate cosine similarity. Higher scores = greater semantic similarity.

embeddings_matrix = np.array(result)
similarity_matrix = cosine_similarity(embeddings_matrix)

for i, text1 in enumerate(texts):
    for j in range(i + 1, len(texts)):
        text2 = texts[j]
        similarity = similarity_matrix[i, j]
        print(f"Similarity between '{text1}' and '{text2}': {similarity:.4f}")
The following shows an example output from this code snippet:


Similarity between 'What is the meaning of life?' and 'What is the purpose of existence?': 0.9481

Similarity between 'What is the meaning of life?' and 'How do I bake a cake?': 0.7471

Similarity between 'What is the purpose of existence?' and 'How do I bake a cake?': 0.7371


Supported task types
Task type	Description	Examples
SEMANTIC_SIMILARITY	Embeddings optimized to assess text similarity.	Recommendation systems, duplicate detection
CLASSIFICATION	Embeddings optimized to classify texts according to preset labels.	Sentiment analysis, spam detection
CLUSTERING	Embeddings optimized to cluster texts based on their similarities.	Document organization, market research, anomaly detection
RETRIEVAL_DOCUMENT	Embeddings optimized for document search.	Indexing articles, books, or web pages for search.
RETRIEVAL_QUERY	Embeddings optimized for general search queries. Use RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for documents to be retrieved.	Custom search
CODE_RETRIEVAL_QUERY	Embeddings optimized for retrieval of code blocks based on natural language queries. Use CODE_RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for code blocks to be retrieved.	Code suggestions and search
QUESTION_ANSWERING	Embeddings for questions in a question-answering system, optimized for finding documents that answer the question. Use QUESTION_ANSWERING for questions; RETRIEVAL_DOCUMENT for documents to be retrieved.	Chatbox
FACT_VERIFICATION	Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement. Use FACT_VERIFICATION for the target text; RETRIEVAL_DOCUMENT for documents to be retrieved	Automated fact-checking systems


----------


     
Document search with embeddings


In this tutorial, you'll use embeddings to perform document search over a set of documents to ask questions related to the Google Car.


from google import  genai
from google.colab import userdata

GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY)
     
Embedding generation
In this section, you will see how to generate embeddings for a piece of text using the embeddings from the Gemini API.

See the Embeddings quickstart to learn more about the task_type parameter used below.


from google.genai import types

title = "The next generation of AI for developers and Google Workspace"
sample_text = """
    Title: The next generation of AI for developers and Google Workspace
    Full article:
    Gemini API & Google AI Studio: An approachable way to explore and
    prototype with generative AI applications
"""

EMBEDDING_MODEL_ID = MODEL_ID = "gemini-embedding-001"  # @param ["gemini-embedding-001", "text-embedding-004"] {"allow-input": true, "isTemplate": true}
embedding = client.models.embed_content(
        model=EMBEDDING_MODEL_ID,
        contents=sample_text,
        config=types.EmbedContentConfig(
            task_type="retrieval_document",
            title=title
    ))

print(embedding)
     
embeddings=[ContentEmbedding(
  values=[
    -0.019380787,
    0.015025399,
    0.006310311,
    -0.057478663,
    0.011998727,
    <... 3067 more items ...>,
  ]
)] metadata=None
Building an embeddings database
Here are three sample texts to use to build the embeddings database. You will use the Gemini API to create embeddings of each of the documents. Turn them into a dataframe for better visualization.


DOCUMENT1 = {
    "title": "Operating the Climate Control System",
    "content": "Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it."}
DOCUMENT2 = {
    "title": "Touchscreen",
    "content": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."}
DOCUMENT3 = {
    "title": "Shifting Gears",
    "content": "Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions."}

documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]
     
Organize the contents of the dictionary into a dataframe for better visualization.


import pandas as pd

df = pd.DataFrame(documents)
df.columns = ['Title', 'Text']
df
     
Title	Text
0	Operating the Climate Control System	Your Googlecar has a climate control system th...
1	Touchscreen	Your Googlecar has a large touchscreen display...
2	Shifting Gears	Your Googlecar has an automatic transmission. ...
Get the embeddings for each of these bodies of text. Add this information to the dataframe.


# Get the embeddings of each text and add to an embeddings column in the dataframe
def embed_fn(title, text):
  response = client.models.embed_content(
        model=EMBEDDING_MODEL_ID,
        contents=text,
        config=types.EmbedContentConfig(
            task_type="retrieval_document",
            title=title
        )
    )

  return response.embeddings[0].values

df['Embeddings'] = df.apply(lambda row: embed_fn(row['Title'], row['Text']), axis=1)
df
     
Title	Text	Embeddings
0	Operating the Climate Control System	Your Googlecar has a climate control system th...	[0.02483931, -0.003871694, 0.013593362, -0.031...
1	Touchscreen	Your Googlecar has a large touchscreen display...	[0.008149438, -0.0013574613, -0.0029458047, -0...
2	Shifting Gears	Your Googlecar has an automatic transmission. ...	[0.009464946, 0.022619268, -0.0036155856, -0.0...
Document search with Q&A
Now that the embeddings are generated, let's create a Q&A system to search these documents. You will ask a question about hyperparameter tuning, create an embedding of the question, and compare it against the collection of embeddings in the dataframe.

The embedding of the question will be a vector (list of float values), which will be compared against the vector of the documents using the dot product. This vector returned from the API is already normalized. The dot product represents the similarity in direction between two vectors.

The values of the dot product can range between -1 and 1, inclusive. If the dot product between two vectors is 1, then the vectors are in the same direction. If the dot product value is 0, then these vectors are orthogonal, or unrelated, to each other. Lastly, if the dot product is -1, then the vectors point in the opposite direction and are not similar to each other.

Note, with the new embeddings model (gemini-embedding-001), specify the task type as QUERY for user query and DOCUMENT when embedding a document text.

Task Type	Description
RETRIEVAL_QUERY	Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT	Specifies the given text is a document in a search/retrieval setting.

query = "How to shift gears in the Google car?"

request = client.models.embed_content(
    model=EMBEDDING_MODEL_ID,
    contents=query,
    config=types.EmbedContentConfig(
        task_type="RETRIEVAL_DOCUMENT",
        )
)
     
Use the find_best_passage function to calculate the dot products, and then sort the dataframe from the largest to smallest dot product value to retrieve the relevant passage out of the database.


import numpy as np

def find_best_passage(query, dataframe):
  """
  Compute the distances between the query and each document in the dataframe
  using the dot product.
  """
  query_embedding = client.models.embed_content(
      model=EMBEDDING_MODEL_ID,
      contents=query,
      config=types.EmbedContentConfig(
          task_type="retrieval_document",
          )
  )

  dot_products = np.dot(
      np.stack(dataframe['Embeddings']),
      query_embedding.embeddings[0].values
  )
  idx = np.argmax(dot_products)
  return dataframe.iloc[idx]['Text'] # Return text from index with max value
     
View the most relevant document from the database:


from IPython.display import Markdown

passage = find_best_passage(query, df)
Markdown(passage)
     
Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position. Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.

Question and Answering Application
Let's try to use the text generation API to create a Q & A system. Input your own custom data below to create a simple question and answering example. You will still use the dot product as a metric of similarity.


import textwrap

def make_prompt(query, relevant_passage):
  escaped = (
      relevant_passage
      .replace("'", "")
      .replace('"', "")
      .replace("\n", " ")
  )
  prompt = textwrap.dedent("""
    You are a helpful and informative bot that answers questions using text
    from the reference passage included below. Be sure to respond in a
    complete sentence, being comprehensive, including all relevant
    background information.

    However, you are talking to a non-technical audience, so be sure to
    break down complicated concepts and strike a friendly and conversational
    tone. If the passage is irrelevant to the answer, you may ignore it.

    QUESTION: '{query}'
    PASSAGE: '{relevant_passage}'

    ANSWER:
  """).format(query=query, relevant_passage=escaped)


  return prompt
     

prompt = make_prompt(query, passage)
Markdown(prompt)
     
You are a helpful and informative bot that answers questions using text from the reference passage included below. Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.

However, you are talking to a non-technical audience, so be sure to break down complicated concepts and strike a friendly and conversational tone. If the passage is irrelevant to the answer, you may ignore it.

QUESTION: 'How to shift gears in the Google car?' PASSAGE: 'Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position. Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.'

ANSWER:

Choose one of the Gemini content generation models in order to find the answer to your query.


MODEL_ID = "gemini-2.5-flash" # @param ["gemini-2.5-flash-lite", "gemini-2.5-flash-lite-preview-09-2025", "gemini-2.5-flash", "gemini-2.5-flash-preview-09-2025", "gemini-2.5-pro"] {"allow-input":true, isTemplate: true}
answer = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
)
     

Markdown(answer.text)
     
Good news! Your Googlecar actually has an automatic transmission, which means shifting gears is super simple â€“ you just move the shift lever to the spot you need! For instance, if you're parked and want the car to stay put, you'll put it in 'Park' because that locks the wheels. When you need to back up, you'll choose 'Reverse.' If you're stopped at a traffic light or in slow traffic and don't want the car to roll, 'Neutral' is the spot; the car won't move unless you press the gas pedal. To drive forward, you'll simply select 'Drive.' And for those times when you're driving in snow or really slippery conditions, there's a 'Low' position to help you out.

Next steps
Check out the embeddings quickstart to learn more, and browse the cookbook for more examples.
------------

I want a display text "listen" Speaker/speak icon below the output 
For both models

Use "gemini-2.5-flash-preview-tts" English language 
use exactly this "gemini-2.5-flash-preview-tts"


----sample code----

# To run this code you need to install the following dependencies:
# pip install google-genai

import base64
import mimetypes
import os
import re
import struct
from google import genai
from google.genai import types


def save_binary_file(file_name, data):
    f = open(file_name, "wb")
    f.write(data)
    f.close()
    print(f"File saved to to: {file_name}")


def generate():
    client = genai.Client(
        api_key=os.environ.get("GEMINI_API_KEY"),
    )

    model = "gemini-2.5-flash-preview-tts"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""-----insert Output of previous LLM prompt here-----"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        temperature=1,
        response_modalities=[
            "audio",
        ],
        speech_config=types.SpeechConfig(
            voice_config=types.VoiceConfig(
                prebuilt_voice_config=types.PrebuiltVoiceConfig(
                    voice_name="Zephyr"
                )
            )
        ),
    )

    file_index = 0
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        if (
            chunk.candidates is None
            or chunk.candidates[0].content is None
            or chunk.candidates[0].content.parts is None
        ):
            continue
        if chunk.candidates[0].content.parts[0].inline_data and chunk.candidates[0].content.parts[0].inline_data.data:
            file_name = f"ENTER_FILE_NAME_{file_index}"
            file_index += 1
            inline_data = chunk.candidates[0].content.parts[0].inline_data
            data_buffer = inline_data.data
            file_extension = mimetypes.guess_extension(inline_data.mime_type)
            if file_extension is None:
                file_extension = ".wav"
                data_buffer = convert_to_wav(inline_data.data, inline_data.mime_type)
            save_binary_file(f"{file_name}{file_extension}", data_buffer)
        else:
            print(chunk.text)

def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:
    """Generates a WAV file header for the given audio data and parameters.

    Args:
        audio_data: The raw audio data as a bytes object.
        mime_type: Mime type of the audio data.

    Returns:
        A bytes object representing the WAV file header.
    """
    parameters = parse_audio_mime_type(mime_type)
    bits_per_sample = parameters["bits_per_sample"]
    sample_rate = parameters["rate"]
    num_channels = 1
    data_size = len(audio_data)
    bytes_per_sample = bits_per_sample // 8
    block_align = num_channels * bytes_per_sample
    byte_rate = sample_rate * block_align
    chunk_size = 36 + data_size  # 36 bytes for header fields before data chunk size

    # http://soundfile.sapp.org/doc/WaveFormat/

    header = struct.pack(
        "<4sI4s4sIHHIIHH4sI",
        b"RIFF",          # ChunkID
        chunk_size,       # ChunkSize (total file size - 8 bytes)
        b"WAVE",          # Format
        b"fmt ",          # Subchunk1ID
        16,               # Subchunk1Size (16 for PCM)
        1,                # AudioFormat (1 for PCM)
        num_channels,     # NumChannels
        sample_rate,      # SampleRate
        byte_rate,        # ByteRate
        block_align,      # BlockAlign
        bits_per_sample,  # BitsPerSample
        b"data",          # Subchunk2ID
        data_size         # Subchunk2Size (size of audio data)
    )
    return header + audio_data

def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:
    """Parses bits per sample and rate from an audio MIME type string.

    Assumes bits per sample is encoded like "L16" and rate as "rate=xxxxx".

    Args:
        mime_type: The audio MIME type string (e.g., "audio/L16;rate=24000").

    Returns:
        A dictionary with "bits_per_sample" and "rate" keys. Values will be
        integers if found, otherwise None.
    """
    bits_per_sample = 16
    rate = 24000

    # Extract rate from parameters
    parts = mime_type.split(";")
    for param in parts: # Skip the main type part
        param = param.strip()
        if param.lower().startswith("rate="):
            try:
                rate_str = param.split("=", 1)[1]
                rate = int(rate_str)
            except (ValueError, IndexError):
                # Handle cases like "rate=" with no value or non-integer value
                pass # Keep rate as default
        elif param.startswith("audio/L"):
            try:
                bits_per_sample = int(param.split("L", 1)[1])
            except (ValueError, IndexError):
                pass # Keep bits_per_sample as default if conversion fails

    return {"bits_per_sample": bits_per_sample, "rate": rate}


if __name__ == "__main__":
    generate()

----sample code huggingface----
import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="hf-inference",
    api_key=os.environ["HF_TOKEN"],
)

result = client.sentence_similarity(
    {
    "source_sentence": "That is a happy person",
    "sentences": [
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
},
    model="google/embeddinggemma-300m",
)




----sample code huggingface----

import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="featherless-ai",
    api_key=os.environ["HF_TOKEN"],
)

completion = client.chat.completions.create(
    model="Qwen/Qwen3-1.7B",
    messages=[
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
)

print(completion.choices[0].message)


----------
test the application, check for any errors in code
use Playwright MCP 

Also You have full ADMIN access of this repo, and full internet access

----END---
