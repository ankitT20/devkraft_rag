# DevKraft RAG Application

A simple Retrieval-Augmented Generation (RAG) system with a Streamlit UI, FastAPI backend, and LangChain integration.

## Features

- ğŸ¤– **Dual Model Support**: Choose between Gemini Cloud (gemini-2.5-flash) or Local LMStudio/HF (qwen3-1.7b)
- ğŸ“š **Document Ingestion**: Upload and process multiple document types (TXT, PDF, DOCX, MD)
- ğŸ’¾ **Vector Storage**: Dual storage with Qdrant Cloud and Docker
- ğŸ’¬ **Chat History**: Persistent chat sessions stored in MongoDB Atlas (with JSON file fallback)
- ğŸ§  **Thinking Display**: View model reasoning process (qwen3)
- ğŸ” **RAG Pipeline**: Semantic search and context-aware responses

## Project Structure

```
devkraft_rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/              # Core functionality
â”‚   â”‚   â”œâ”€â”€ embeddings.py  # Embedding services (Gemini, Local/HF)
â”‚   â”‚   â”œâ”€â”€ llm.py         # LLM services (Gemini, Local/HF)
â”‚   â”‚   â”œâ”€â”€ storage.py     # Qdrant vector storage
â”‚   â”‚   â””â”€â”€ chat_storage.py # MongoDB chat history storage
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ document_processor.py  # Document loading and chunking
â”‚   â”‚   â”œâ”€â”€ ingestion.py   # Document ingestion pipeline
â”‚   â”‚   â””â”€â”€ rag.py         # RAG query processing
â”‚   â”œâ”€â”€ models/            # Pydantic schemas
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ utils/             # Utilities
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â””â”€â”€ main.py            # FastAPI application
â”œâ”€â”€ info/                  # Documentation and API collections
â”‚   â”œâ”€â”€ rag.postman_collection.json  # Postman API collection
â”‚   â”œâ”€â”€ rag.postman_environment.json # Postman environment file
â”‚   â”œâ”€â”€ .env.example       # Environment variable template
â”‚   â””â”€â”€ architecture-simple.puml     # System architecture diagram
â”œâ”€â”€ streamlit_app.py       # Streamlit UI
â”œâ”€â”€ generate_embeddings/   # Document upload folder
â”œâ”€â”€ user_chat/             # Chat history storage
â”œâ”€â”€ logs/                  # Application logs
â””â”€â”€ requirements.txt       # Dependencies
```

## Setup

### 1. Environment Variables

Set the following environment variables:

```bash
export GEMINI_API_KEY="your_gemini_api_key"
export QDRANT_API_KEY="your_qdrant_api_key"
export HF_TOKEN="your_huggingface_token"
export MONGO_URI="mongodb+srv://username:password@cluster.mongodb.net/?retryWrites=true"
```

Or create a `.env` file (see `info/.env.example`).

**MongoDB Atlas Setup (Optional):**
- If `MONGO_URI` is provided, chat history will be stored in MongoDB Atlas
- The application automatically appends `&w=majority&appName=ragcluster` to the URI
- If MongoDB is unavailable or not configured, the app falls back to JSON files in `user_chat/`
- No code changes needed - fallback is automatic

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Optional: Start Qdrant Docker

The application will automatically check for existing Qdrant Docker containers and resume them. If no containers exist, you can create one:

```bash
docker run -d -p 6333:6333 qdrant/qdrant
```

The application will fall back to Qdrant Cloud if Docker is not available.

## Running the Application

### Quick Start (Recommended)

Use the provided startup script to start both API and UI:

```bash
./start.sh
```

This will automatically:
- Check for and resume existing Qdrant Docker containers (if any)
- Start the FastAPI backend on port 8000
- Start the Streamlit UI on port 8501
- Append logs to `logs/uvicorn_YYYYMMDD.log` and `logs/streamlit_YYYYMMDD.log`

To stop the application:

```bash
pkill -f uvicorn
pkill -f streamlit
```

### Manual Start

Alternatively, you can start services manually:

**Terminal 1 - FastAPI Backend:**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 - Streamlit UI:**
```bash
streamlit run streamlit_app.py
```

### Access Points

- **Streamlit UI**: http://localhost:8501
- **FastAPI Backend**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs

## Usage

### Model Selection

Choose between two models in the sidebar dropdown:

1. **Gemini Cloud** (Default):
   - Embedding: gemini-embedding-001 (3072 dimensions)
   - LLM: gemini-2.5-flash
   - Storage: Qdrant Cloud

2. **Qwen3 Local**:
   - Embedding: embeddinggemma-300m (768 dimensions) with HF fallback
   - LLM: qwen/qwen3-1.7b with HF fallback
   - Storage: Qdrant Docker with Cloud replication

### Upload Documents

1. Click "Upload Document" in sidebar
2. Select a file (TXT, PDF, DOCX, or MD)
3. Click "Upload" button
4. Document will be processed and stored in vector databases

### Chat

1. Type your question in the chat input
2. Press Enter to send
3. View the AI response
4. For Qwen3 model, expand "Show Thinking" to see reasoning

### Chat History

- Recent chats appear in the sidebar
- Click a chat to load it
- Click "New Chat" to start fresh

## API Endpoints

- `GET /` - Health check
- `GET /health` - Health status
- `POST /query` - Process RAG query
- `POST /query-stream` - Process RAG query with streaming response
- `POST /upload` - Upload and ingest document
- `POST /ingest-all` - Ingest all documents in folder
- `GET /chats` - Get recent chat sessions
- `GET /chat/{chat_id}` - Get full chat history
- `POST /tts` - Convert text to speech

Visit http://localhost:8000/docs for interactive API documentation.

### Testing with Postman

A complete Postman collection is available in the `info/` folder:

1. **Collection**: `info/rag.postman_collection.json` - Contains all API endpoints with pre-filled examples
2. **Environment**: `info/rag.postman_environment.json` - Environment variables for API keys and configuration

To use:
1. Import both files into Postman
2. Update the environment variables with your actual API keys (replace `****`)
3. Start testing the APIs immediately

## Configuration

All application settings are centralized in `app/config.py`. You can customize:

- API keys and tokens (via environment variables or .env file)
- MongoDB connection URI for chat history storage
- Chunk size and overlap for document processing
- Model names for embeddings and LLM
- Qdrant URLs and collection names
- File paths and directories
- LM Studio configuration

See `app/config.py` for all available settings.

### Chat History Storage

The application uses a dual-storage strategy for chat history:

1. **Primary Storage**: MongoDB Atlas (when `MONGO_URI` is configured)
   - Persistent cloud storage
   - Better query performance for large chat histories
   - Automatic indexing on `chat_id` and `updated_at`

2. **Fallback Storage**: JSON files in `user_chat/` folder
   - Activated when MongoDB is unavailable or not configured
   - Zero-configuration required
   - Compatible with existing chat history files

The application automatically handles the fallback logic. You don't need to modify any code.

## Logging

All logs are stored in the `logs/` folder with automatic date-based naming:

- `app_logs_YYYYMMDD.log` - All application logs (INFO level and above)
- `errors_YYYYMMDD.log` - Error logs only (ERROR level)
- `uvicorn_YYYYMMDD.log` - FastAPI/Uvicorn server logs (appended daily)
- `streamlit_YYYYMMDD.log` - Streamlit UI logs (appended daily)

Logs are appended when the application is restarted multiple times on the same day. Logs include detailed timestamps, function names, and line numbers for debugging.

## Embedding Details

### Gemini (Cloud)
- **Model**: gemini-embedding-001
- **Dimensions**: 3072
- **Normalization**: Pre-normalized by API
- **Task Type**: RETRIEVAL_DOCUMENT for docs, RETRIEVAL_QUERY for queries

### Local (LMStudio/HF)
- **Model**: embeddinggemma-300m
- **Dimensions**: 768
- **Normalization**: Manual normalization required
- **Fallback**: HuggingFace API if LMStudio unavailable

## Document Processing

- **Chunking**: Semantic chunking with LangChain
- **Chunk Size**: 2000 characters
- **Overlap**: 400 characters
- **Supported Formats**: TXT, PDF, DOCX, MD

## Storage Strategy

Documents are processed and stored in both:
1. **Qdrant Cloud** with Gemini embeddings (3072d)
2. **Qdrant Docker** with Local embeddings (768d)

After processing, files are moved to:
- `generate_embeddings/stored/` - Both storages succeeded
- `generate_embeddings/stored_in_q_cloud_only/` - Cloud only
- `generate_embeddings/stored_in_q_docker_only/` - Docker only

### Make By Ankit Tayal
